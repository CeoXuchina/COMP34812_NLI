{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11229924,"sourceType":"datasetVersion","datasetId":7014441},{"sourceId":11256300,"sourceType":"datasetVersion","datasetId":7034734}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport string\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nfrom IPython.display import display\nimport itertools\nimport joblib\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:29:56.486962Z","iopub.execute_input":"2025-04-03T21:29:56.487255Z","iopub.status.idle":"2025-04-03T21:29:56.492542Z","shell.execute_reply.started":"2025-04-03T21:29:56.487234Z","shell.execute_reply":"2025-04-03T21:29:56.491656Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/trainingdata/train.csv')\ndf_valid = pd.read_csv('/kaggle/input/trainingdata/dev.csv')\n\ndf_train[:10]\nprint(\"Train columns:\", df_train.columns)\nprint(\"Valid columns:\", df_valid.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:00:15.818410Z","iopub.execute_input":"2025-04-03T21:00:15.818725Z","iopub.status.idle":"2025-04-03T21:00:15.924721Z","shell.execute_reply.started":"2025-04-03T21:00:15.818695Z","shell.execute_reply":"2025-04-03T21:00:15.923987Z"}},"outputs":[{"name":"stdout","text":"Train columns: Index(['premise', 'hypothesis', 'label'], dtype='object')\nValid columns: Index(['premise', 'hypothesis', 'label'], dtype='object')\nTest columns: Index(['premise', 'hypothesis'], dtype='object')\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# stop_words = set(stopwords.words('english'))\ndef pre_clean(text):\n    # Case folding\n    text = text.lower()\n    # keep english words,numbers and space \n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    words = text.split()\n    # Remove stop words\n    # words = [word for word in words if word not in stop_words]\n    return ' '.join(words)\ndf_train['premise'] = df_train['premise'].apply(pre_clean)\ndf_train['hypothesis'] = df_train['hypothesis'].apply(pre_clean)\ndf_valid['premise'] = df_valid['premise'].apply(pre_clean)\ndf_valid['hypothesis'] = df_valid['hypothesis'].apply(pre_clean) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:00:20.830506Z","iopub.execute_input":"2025-04-03T21:00:20.830808Z","iopub.status.idle":"2025-04-03T21:00:21.064324Z","shell.execute_reply.started":"2025-04-03T21:00:20.830786Z","shell.execute_reply":"2025-04-03T21:00:21.063431Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"X_train = df_train['premise'] + ' [SEP] ' + df_train['hypothesis'].astype(str)\ny_train = df_train['label']\nX_valid = df_valid['premise'] + ' [SEP] ' + df_valid['hypothesis'].astype(str)\ny_valid = df_valid['label']\n\ndiff_results = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:00:23.878130Z","iopub.execute_input":"2025-04-03T21:00:23.878444Z","iopub.status.idle":"2025-04-03T21:00:23.899139Z","shell.execute_reply.started":"2025-04-03T21:00:23.878423Z","shell.execute_reply":"2025-04-03T21:00:23.898418Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"param_grid = {\n    \"max_features\": [3000, 5000 ,10000],\n    \"ngrams\": [(1, 1), (1, 2),(1,3)],\n    \"Cs\": [0.2, 0.5, 1.0],\n    \"Alphas\":[0.2, 0.5, 1.0],\n    \"n_estimators\":[30, 50, 100]\n}\nparam_combinations = list(itertools.product(\n    param_grid[\"max_features\"],\n    param_grid[\"ngrams\"],\n    param_grid[\"Cs\"],\n    param_grid[\"Alphas\"],\n    param_grid[\"n_estimators\"],\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:00:25.566888Z","iopub.execute_input":"2025-04-03T21:00:25.567164Z","iopub.status.idle":"2025-04-03T21:00:25.571853Z","shell.execute_reply.started":"2025-04-03T21:00:25.567144Z","shell.execute_reply":"2025-04-03T21:00:25.570955Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 1. TF-IDF + Logistic Regression\nlr_combinations = list(set([\n    (m, ngram, C) for (m, ngram, C, _, _) in param_combinations\n]))\nfor m, ngram, C_val in lr_combinations:\n    vectorizer_lr = TfidfVectorizer(\n                        max_features=m,\n                        ngram_range=ngram,\n                        min_df=2,\n                        max_df=0.9,\n                        sublinear_tf=True\n                        \n                    )\n    # Transform the training and validation data into TF-IDF feature vectors\n    X_train_tfidf_lr = vectorizer_lr.fit_transform(X_train)\n    X_valid_tfidf_lr = vectorizer_lr.transform(X_valid)\n    # Initialize a Logistic Regression\n    reg = LogisticRegression(\n        C = C_val, # regularization strength\n        max_iter=1000, \n        solver='liblinear',\n        verbose = 1\n                    )\n    # Train the Logistic Regression model\n    reg.fit(X_train_tfidf_lr, y_train)\n    y_pred_lr = reg.predict(X_valid_tfidf_lr)\n    diff_results.append({\n        \"Model\": \"TF-IDF + Logistic Regression\",\n        \"max_features\": m,\n        \"ngram_range\": ngram,\n        \"C\": C_val,\n        \"Accuracy\": accuracy_score(y_valid, y_pred_lr),\n        \"F1 Score\": f1_score(y_valid, y_pred_lr, average='weighted')\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:00:27.914918Z","iopub.execute_input":"2025-04-03T21:00:27.915187Z","iopub.status.idle":"2025-04-03T21:01:39.338881Z","shell.execute_reply.started":"2025-04-03T21:00:27.915167Z","shell.execute_reply":"2025-04-03T21:01:39.337960Z"}},"outputs":[{"name":"stdout","text":"[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 2. Bag-of-Words + SVM\n\n # Create a CountVectorizer (BoW) with the current max_features and n-gram settingbow_combinations = list(set([\n    (m, ngram, C) for (m, ngram, C, _, _) in param_combinations\n]))\nfor m, ngram, C_val in bow_combinations:\n     # Create a CountVectorizer (BoW) with the current max_features and n-gram setting\n    vectorizer_bow = CountVectorizer(\n                        max_features=m,\n                        ngram_range=ngram,\n                    )\n    # Transform the training and validation text data into BoW feature vectors\n    X_train_bow = vectorizer_bow.fit_transform(X_train)\n    X_valid_bow = vectorizer_bow.transform(X_valid)\n    # Initialize a Logistic Regression classifier\n    svm = LinearSVC(\n        C = C_val,\n        max_iter=20000\n    )\n    # Train the SVM model on the BoW-transformed training data\n    svm.fit(X_train_bow, y_train)\n    y_pred_svm = svm.predict(X_valid_bow)\n    diff_results.append({\n        \"Model\": \"BoW + SVM\",\n        \"max_features\": m,\n        \"ngram_range\": ngram,\n        \"C\": C_val,\n        \"Accuracy\": accuracy_score(y_valid, y_pred_svm),\n        \"F1 Score\": f1_score(y_valid, y_pred_svm, average='weighted')\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:03:06.738032Z","iopub.execute_input":"2025-04-03T21:03:06.738327Z","iopub.status.idle":"2025-04-03T21:13:13.136177Z","shell.execute_reply.started":"2025-04-03T21:03:06.738305Z","shell.execute_reply":"2025-04-03T21:13:13.135447Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 3. TF-IDF + Naive Bayes\nnb_combinations = list(set([\n    (m, ngram, aa) for (m, ngram, _, aa, _) in param_combinations\n]))\nfor m, ngram,aa,in nb_combinations:\n    # Create a TF-IDF vectorizer with the specified parameters\n    vectorizer_nb = TfidfVectorizer(\n                        max_features=m,\n                        ngram_range=ngram,\n                        min_df=1,\n                        max_df=0.9,\n                        sublinear_tf=True\n                    )\n    # Transform training and validation data into TF-IDF feature vectors\n    X_train_tfidf_nb = vectorizer_nb.fit_transform(X_train)\n    X_valid_tfidf_nb = vectorizer_nb.transform(X_valid)\n    try:    \n        nb = MultinomialNB(\n            fit_prior=False, \n            alpha=aa, # smoothing parameter \n            force_alpha=True\n        )\n        # Train the Naive Bayes classifier\n        nb.fit(X_train_tfidf_nb, y_train)\n        y_pred_nb = nb.predict(X_valid_tfidf_nb)\n        \n        diff_results.append({\n            \"Model\": \"TF-IDF + Naive Bayes\",\n            \"max_features\": m,\n            \"ngram_range\": ngram,\n            \"alpha\": aa,\n            \"Accuracy\": accuracy_score(y_valid, y_pred_nb),\n            \"F1 Score\": f1_score(y_valid, y_pred_nb, average='weighted')\n        })\n    except Exception as e:\n        print(f\"Skipped alpha={aa}, m={m}, ngram={ngram}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:13:13.137239Z","iopub.execute_input":"2025-04-03T21:13:13.137618Z","iopub.status.idle":"2025-04-03T21:14:21.653991Z","shell.execute_reply.started":"2025-04-03T21:13:13.137564Z","shell.execute_reply":"2025-04-03T21:14:21.652938Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# 4. TF-IDF + Random Forest\nrf_combinations = list(set([\n    (m, ngram, est) for (m, ngram, _, _, est) in param_combinations\n]))\nfor m, ngram,est in rf_combinations:\n    # Create a TF-IDF vectorizer with specified max_features and n-gram range\n    vectorizer_rf = TfidfVectorizer(\n        max_features=m,\n        ngram_range=ngram,\n        min_df=2,\n        max_df=0.9,\n        sublinear_tf=True\n    )\n    # Transform the text data into TF-IDF feature vectors\n    X_train_tfidf_rf = vectorizer_rf.fit_transform(X_train)\n    X_valid_tfidf_rf = vectorizer_rf.transform(X_valid)\n    # Initialize the Random Forest classifier\n    rf = RandomForestClassifier(\n        n_estimators= est, \n        random_state= 42,\n        class_weight='balanced'\n    )\n    # Train the Random Forest model\n    rf.fit(X_train_tfidf_rf, y_train)\n    y_pred_rf = rf.predict(X_valid_tfidf_rf)\n    \n    diff_results.append({\n        \"Model\": \"TF-IDF + Random Forest\",\n        \"max_features\": m,\n        \"ngram_range\": ngram,\n        \"n_ests\":est,\n        \"Accuracy\": accuracy_score(y_valid, y_pred_rf),\n        \"F1 Score\": f1_score(y_valid, y_pred_rf, average='weighted')\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:14:46.299337Z","iopub.execute_input":"2025-04-03T21:14:46.299682Z","iopub.status.idle":"2025-04-03T21:25:33.336814Z","shell.execute_reply.started":"2025-04-03T21:14:46.299654Z","shell.execute_reply":"2025-04-03T21:25:33.336105Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame(diff_results)\n\nif \"C\" not in df.columns:\n    df[\"C\"] = None\nif \"alpha\" not in df.columns:\n    df[\"alpha\"] = None\nif \"n_ests\" not in df.columns:\n    df[\"n_ests\"] = None\n\n# Top 3 for every model\nsummary_df = (\n    df.sort_values(by=\"Accuracy\", ascending=False)\n      .groupby(\"Model\")\n      .head(3)\n      .reset_index(drop=True)\n)\n\nsummary_df = summary_df[[\n    \"Model\",\n    \"max_features\",\n    \"C\",\n    \"ngram_range\",\n    \"alpha\",\n    \"n_ests\",\n    \"Accuracy\",\n    \"F1 Score\"\n]]\nsummary_df.columns = [\"Model\", \"Max_features\", \"C\", \"Ngram_range\",\"Alpha\",\"n_ests\",\"Accuracy\", \"F1 Score\"]\n\nsummary_df[\"Accuracy\"] = summary_df[\"Accuracy\"].round(4)\nsummary_df[\"F1 Score\"] = summary_df[\"F1 Score\"].round(4)\n\ndisplay(summary_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:30:05.181004Z","iopub.execute_input":"2025-04-03T21:30:05.181289Z","iopub.status.idle":"2025-04-03T21:30:05.204291Z","shell.execute_reply.started":"2025-04-03T21:30:05.181267Z","shell.execute_reply":"2025-04-03T21:30:05.203515Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                           Model  Max_features    C Ngram_range  Alpha  \\\n0         TF-IDF + Random Forest          5000  NaN      (1, 3)    NaN   \n1         TF-IDF + Random Forest          3000  NaN      (1, 2)    NaN   \n2         TF-IDF + Random Forest          3000  NaN      (1, 2)    NaN   \n3   TF-IDF + Logistic Regression         10000  0.2      (1, 3)    NaN   \n4   TF-IDF + Logistic Regression          5000  0.5      (1, 2)    NaN   \n5   TF-IDF + Logistic Regression          3000  0.2      (1, 2)    NaN   \n6           TF-IDF + Naive Bayes          3000  NaN      (1, 2)    0.2   \n7           TF-IDF + Naive Bayes          3000  NaN      (1, 2)    0.5   \n8           TF-IDF + Naive Bayes          3000  NaN      (1, 3)    1.0   \n9                      BoW + SVM          3000  0.2      (1, 3)    NaN   \n10                     BoW + SVM          3000  0.5      (1, 3)    NaN   \n11                     BoW + SVM          3000  1.0      (1, 3)    NaN   \n\n    n_ests  Accuracy  F1 Score  \n0    100.0    0.6263    0.6239  \n1     50.0    0.6237    0.6225  \n2    100.0    0.6222    0.6203  \n3      NaN    0.6177    0.6115  \n4      NaN    0.6177    0.6154  \n5      NaN    0.6176    0.6128  \n6      NaN    0.5917    0.5917  \n7      NaN    0.5909    0.5909  \n8      NaN    0.5904    0.5903  \n9      NaN    0.5904    0.5900  \n10     NaN    0.5876    0.5872  \n11     NaN    0.5858    0.5854  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Max_features</th>\n      <th>C</th>\n      <th>Ngram_range</th>\n      <th>Alpha</th>\n      <th>n_ests</th>\n      <th>Accuracy</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TF-IDF + Random Forest</td>\n      <td>5000</td>\n      <td>NaN</td>\n      <td>(1, 3)</td>\n      <td>NaN</td>\n      <td>100.0</td>\n      <td>0.6263</td>\n      <td>0.6239</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TF-IDF + Random Forest</td>\n      <td>3000</td>\n      <td>NaN</td>\n      <td>(1, 2)</td>\n      <td>NaN</td>\n      <td>50.0</td>\n      <td>0.6237</td>\n      <td>0.6225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TF-IDF + Random Forest</td>\n      <td>3000</td>\n      <td>NaN</td>\n      <td>(1, 2)</td>\n      <td>NaN</td>\n      <td>100.0</td>\n      <td>0.6222</td>\n      <td>0.6203</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TF-IDF + Logistic Regression</td>\n      <td>10000</td>\n      <td>0.2</td>\n      <td>(1, 3)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.6177</td>\n      <td>0.6115</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TF-IDF + Logistic Regression</td>\n      <td>5000</td>\n      <td>0.5</td>\n      <td>(1, 2)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.6177</td>\n      <td>0.6154</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>TF-IDF + Logistic Regression</td>\n      <td>3000</td>\n      <td>0.2</td>\n      <td>(1, 2)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.6176</td>\n      <td>0.6128</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>TF-IDF + Naive Bayes</td>\n      <td>3000</td>\n      <td>NaN</td>\n      <td>(1, 2)</td>\n      <td>0.2</td>\n      <td>NaN</td>\n      <td>0.5917</td>\n      <td>0.5917</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>TF-IDF + Naive Bayes</td>\n      <td>3000</td>\n      <td>NaN</td>\n      <td>(1, 2)</td>\n      <td>0.5</td>\n      <td>NaN</td>\n      <td>0.5909</td>\n      <td>0.5909</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>TF-IDF + Naive Bayes</td>\n      <td>3000</td>\n      <td>NaN</td>\n      <td>(1, 3)</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>0.5904</td>\n      <td>0.5903</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>BoW + SVM</td>\n      <td>3000</td>\n      <td>0.2</td>\n      <td>(1, 3)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.5904</td>\n      <td>0.5900</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>BoW + SVM</td>\n      <td>3000</td>\n      <td>0.5</td>\n      <td>(1, 3)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.5876</td>\n      <td>0.5872</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>BoW + SVM</td>\n      <td>3000</td>\n      <td>1.0</td>\n      <td>(1, 3)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.5858</td>\n      <td>0.5854</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# best model\nvectorizer_rf_best = TfidfVectorizer(\n        max_features = 5000,\n        ngram_range=(1,3),\n        min_df=2,\n        max_df=0.9,\n        sublinear_tf=True\n    )\nX_train_tfidf_rf_best = vectorizer_rf_best.fit_transform(X_train)\nX_valid_tfidf_rf_best = vectorizer_rf_best.transform(X_valid)\n\nrf_best = RandomForestClassifier(\n    n_estimators= 100, \n    random_state= 42,\n    class_weight='balanced'\n)\nrf_best.fit(X_train_tfidf_rf_best, y_train)\ny_pred_rf_best = rf_best.predict(X_valid_tfidf_rf_best)\n    \nprint(\"Accuracy:\", accuracy_score(y_valid, y_pred_rf_best))\nprint(\"F1 Score (weighted):\", f1_score(y_valid, y_pred_rf_best, average='weighted'))\nprint(\"\\nClassification Report:\\n\", classification_report(y_valid, y_pred_rf_best))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:28:35.236996Z","iopub.execute_input":"2025-04-03T21:28:35.237308Z","iopub.status.idle":"2025-04-03T21:29:15.958344Z","shell.execute_reply.started":"2025-04-03T21:28:35.237281Z","shell.execute_reply":"2025-04-03T21:29:15.957341Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.6263361045130641\nF1 Score (weighted): 0.6238936747518368\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.63      0.55      0.59      3258\n           1       0.62      0.70      0.66      3478\n\n    accuracy                           0.63      6736\n   macro avg       0.63      0.62      0.62      6736\nweighted avg       0.63      0.63      0.62      6736\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Save best model\n# Save the TF-IDF vectorizer\njoblib.dump(vectorizer_rf_best, 'vectorizer_rf_best.pkl')\n\n# Save the trained RandomForest model\njoblib.dump(rf_best, 'rf_best_model.pkl')\n\nprint(\"The model and vectorizer have been saved successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:30:16.702823Z","iopub.execute_input":"2025-04-03T21:30:16.703111Z","iopub.status.idle":"2025-04-03T21:30:19.716975Z","shell.execute_reply.started":"2025-04-03T21:30:16.703089Z","shell.execute_reply":"2025-04-03T21:30:19.716217Z"}},"outputs":[{"name":"stdout","text":"The model and vectorizer have been saved successfully\n","output_type":"stream"}],"execution_count":16}]}