{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11267701,"sourceType":"datasetVersion","datasetId":7032975}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0fe8ce62","cell_type":"markdown","source":"# RoBERTa Training for NLI","metadata":{}},{"id":"1ddeb327","cell_type":"code","source":"import pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T11:42:28.021185Z","iopub.execute_input":"2025-04-04T11:42:28.021540Z","iopub.status.idle":"2025-04-04T11:42:50.553635Z","shell.execute_reply.started":"2025-04-04T11:42:28.021514Z","shell.execute_reply":"2025-04-04T11:42:50.552830Z"}},"outputs":[],"execution_count":1},{"id":"05a6e0c9","cell_type":"markdown","source":"## Step 1: Load and prepare training & dev datasets","metadata":{}},{"id":"d5235830","cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlu-data-file/train.csv\")\ndev_df = pd.read_csv(\"/kaggle/input/nlu-data-file/dev.csv\")\n\ndataset = Dataset.from_pandas(train_df)\ndev_dataset = Dataset.from_pandas(dev_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T11:42:50.554729Z","iopub.execute_input":"2025-04-04T11:42:50.555332Z","iopub.status.idle":"2025-04-04T11:42:50.817397Z","shell.execute_reply.started":"2025-04-04T11:42:50.555309Z","shell.execute_reply":"2025-04-04T11:42:50.816570Z"}},"outputs":[],"execution_count":2},{"id":"ca2e720c","cell_type":"markdown","source":"## Step 2: Tokenization","metadata":{}},{"id":"eb814c48","cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\ndef tokenize(batch):\n    return tokenizer(batch['premise'], batch['hypothesis'], truncation=True, padding=True, max_length=512)\n\ndataset = dataset.map(tokenize, batched=True)\ndev_dataset = dev_dataset.map(tokenize, batched=True)\n\ndataset.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'label'])\ndev_dataset.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T11:42:50.819108Z","iopub.execute_input":"2025-04-04T11:42:50.819431Z","iopub.status.idle":"2025-04-04T11:43:10.652614Z","shell.execute_reply.started":"2025-04-04T11:42:50.819400Z","shell.execute_reply":"2025-04-04T11:43:10.651941Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cdfcbe6ba064ead9c25568921c2450e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1218173a9ba04a4dbb810aab172c77a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51c8a3c732c844f6baa2074bd67cb933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18175eebc53c4c37984736f9cb66e403"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7cafddb7d474da58289bc1d719b3528"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24432 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d82ee0f9d364d739a199ce1b1da0fb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6736 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e52424b03cf4719aee8814dc612630a"}},"metadata":{}}],"execution_count":3},{"id":"fab98bda","cell_type":"markdown","source":"## Step 3: Define model and training loop","metadata":{}},{"id":"bcefb065","cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = np.argmax(pred.predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=1e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    num_train_epochs=8,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    logging_strategy=\"steps\",\n    report_to=\"none\",              \n    disable_tqdm=False,             \n    log_level=\"info\",               \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    eval_dataset=dev_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T11:43:10.653425Z","iopub.execute_input":"2025-04-04T11:43:10.653686Z","execution_failed":"2025-04-04T11:53:51.337Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d59b94e8277d442bacaa19a8cb585762"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-4-ee74c490dd78>:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nThe following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 24,432\n  Num Epochs = 8\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 12,216\n  Number of trainable parameters = 124,647,170\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1344' max='12216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1344/12216 10:33 < 1:25:29, 2.12 it/s, Epoch 0.88/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"id":"cb958e7f","cell_type":"markdown","source":"## Step 4: Save the model and tokenizer","metadata":{}},{"id":"3d5822c5","cell_type":"code","source":"model.save_pretrained(\"./roberta_nli_model\")\ntokenizer.save_pretrained(\"./roberta_nli_model\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-04T11:53:51.337Z"}},"outputs":[],"execution_count":null}]}